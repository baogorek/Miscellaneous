{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rational Transfer Functions and the Power of Dynamic Representation\n",
    "# On the Limitations of OLS Regression with Lagged Variables\n",
    "# How far will lagged variable regressions go?\n",
    "\n",
    "Every budding modeler at some early point stumbles upon the apparent predictive power of regressing on lagged dependent variables. That it happens long before any formal coursework on the subject is because it is so intuitive. To borrow from [Tobler's first law of geography](https://en.wikipedia.org/wiki/Tobler%27s_first_law_of_geography), \"everything is related to everything else, but near things are more related than distant things,\" a quote which surely holds as true for time as it does for space. And of all the complex factors that come together to create a variable, only a fraction of which we may measure or fully understand, we can rely on the lagged value to be of a similar formula and hense lagged dependent variables encapsulate what we do not know. This is the essense of the reduced dynamic form, and the purpose of this article is to test its limits especially when paired with Ordinary Least Squares regression.\n",
    "\n",
    "Aside from being props to boost a 1-step-ahead R-squared, adding lagged variables takes a model from mapping equation to stochastic process. Suppose, for $z_t = y_t - f(x_t)$ for s\n",
    "\n",
    "## Dynamic challenge: convert a nonlinear, nonstationary, structural time-series model\n",
    "\n",
    "There's a time-series model I've been studying because its nonlinearity, nonstationarity and its encapsulation of latent hypothetical concepts into regression variables. It is the fitness-fatigue model of atheletic performance and has the form\n",
    "\n",
    "$$\n",
    "p_t = \\mu + k_1 \\sum_{i=1}^{t - 1} w_i \\exp\\left(\\frac{-(t - i)}{\\tau_1}\\right) - k_2 \\sum_{i=1}^{t - 1} w_i \\exp\\left(\\frac{-(t - i)}{\\tau_2}\\right) + \\epsilon_t,\n",
    "$$\n",
    "where $p_t$ is a numeric measure of (athletic) performance, $w_t$ is training \"dose\" (i.e., time-weighted training intensity) occuring in time period $t$, and $\\epsilon_t$ is i.i.d. guassian error. The model functions as a linear regression with two nonlinear features, these being:\n",
    "\n",
    "$$\n",
    "h_t =\\sum_{i=1}^{t - 1} w_i \\exp\\left(\\frac{-(t - i)}{\\tau_1}\\right) \\, \\text{and} \\, \\,\n",
    "g_t =\\sum_{i=1}^{t - 1} w_i \\exp\\left(\\frac{-(t - i)}{\\tau_2}\\right),\n",
    "$$\n",
    "latent representations of athletic \"fitness\" and \"fatigue,\" respectively. These complicated features are convolutions of athletic training history with exponential decay but differ in the decay rate. The first convolution sum, $h_t$ typically has a much longer decay and represents fitness. Fatigue, or $g_t$ is much more transient and associated with a faster exponential decay. The regression coefficient on fatigue is typically much larger than for fitness as a counterpoint to its transience.\n",
    "\n",
    "Simple math (done here) show that fitness and fatigue can be put into dynamic form as so:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_t &= \\theta_1 h_{t - 1} + \\theta_1 w_{t - 1}, \\\\\n",
    "g_t &= \\theta_2 g_{t - 1} + \\theta_2 w_{t - 1},\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\theta_1 =  e^{-1 / \\tau_1}$ and $\\theta_2 = e^{-1 / \\tau_2}$.\n",
    "This is a starting point to place this model within a Kalman Filter framework, which works quite well (link). Other approaches to fitting the model are to use brute force nonlinear least squares (link needed) and to use a distributed lag approach with a flexible functional form (link needed). Those methods work well also, but they are complex, and here I pose the central question of the article: *what would happen if you just OLS regressed on lagged variables of performance and training? How far could you get?*\n",
    "\n",
    "The answer is \"pretty far,\" but there are caviats, and blindly applying OLS regression on lagged variables will never get you to the truth.\n",
    "\n",
    "## Rational transfer functions to the rescue\n",
    "\n",
    "If we plug the bivariate dynamic representation of fitness and fatigue into the original model, we arrive at\n",
    "$$\n",
    "p_t = \\mu + k_1 \\theta_1 h_{t - 1} - k_2 \\theta_2 g_{t - 1} + (\\theta_1 + \\theta_2) w_{t - 1} + \\epsilon_t,\n",
    "$$\n",
    "which looks nicer but does us little good since $h_t$ and $g_t$ are unavailable to the modeler. Working a little harder, we can use the \"backshift operator\" $\\text{B}$ defined by $\\text{B} y_t = y_{t-1}$ for arbitrary time-indexed variable $y$, and arrive at \n",
    "$$\n",
    "\\begin{aligned}\n",
    "(1 - \\theta_1 \\text{B}) h_t &= \\theta_1 \\text{B} w_t, \\\\\n",
    "(1 - \\theta_2 \\text{B}) g_t &= \\theta_2 \\text{B} w_t.\n",
    "\\end{aligned}\n",
    "$$\n",
    "Solving for $h_t$ and $g_t$ and plugging back into the original model, we arrive at\n",
    "$$\n",
    "p_t = \\mu + k_1 \\frac{\\theta_1 \\text{B}}{1 - \\theta_1\\text{B}} w_t - k_2 \\frac{\\theta_2 \\text{B}}{1 - \\theta_2\\text{B}} w_t + \\epsilon_t.\n",
    "$$\n",
    "Thus we have two rational transfer functions operating on the exogenous input series $w_t$ (the training load that comes from the coach!). With rational transfer functions, denomonator terms of the form $(1 - \\theta \\text{B})$ correspond to an autoregressive impulse response, i.e., a process with a long memory, and this is a nuissance to us. There is an option to rid ourselves of the denomonator component, but not one without a cost.\n",
    "\n",
    "A \"common filter,\" as discussed in [Identification of Multiple-Input Transfer Function Models](https://www.researchgate.net/publication/276953549_Identification_of_Multiple-Input_Transfer_Function_Models) by Liu & Hanssens (1982) premultiplies the right and left-hand side of a time-series equation by $(1 - \\theta \\text{B})$. It does not change the transfer function weights, so you can apply multiple common filters in succession, and besides causing complexity and losing some rows due to lags, you have not destroyed the relationship between the input and output series.\n",
    "\n",
    "The point is, if we were to use the common filter $(1 - \\theta_1 \\text{B}) (1 - \\theta_2 \\text{B})$, we would be rid of the autoregressive components. Let's see.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(1 - \\theta_1 \\text{B}) (1 - \\theta_2 \\text{B}) p_t = &(1 - \\theta_1 \\text{B}) (1 - \\theta_2 \\text{B}) \\mu + k_1 \\theta_1 (\\text{B} - \\theta_2 \\text{B}^2) w_t - \\\\  &k_2 \\theta_2 (\\text{B} - \\theta_1 \\text{B}^2) w_t +  (1 - \\theta_1 \\text{B}) (1 - \\theta_2 \\text{B})\\epsilon_t.\n",
    "\\end{aligned}\n",
    "$$\n",
    "It still looks ugly, but after expanding the polynomials applying the backshift operations, we arrive at:\n",
    "\n",
    "$$\n",
    "p_t = \\mu^* + (\\theta_1 + \\theta_2) p_{t - 1} - \\theta_1 \\theta_2 p_{t - 2} + (k_1\\theta_1 - k_2 \\theta_2) w_{t-1} - \\theta_1 \\theta_2 (k_1 - k_2) w_{t-2} + \\eta_t,\n",
    "$$\n",
    "$$\n",
    "\\text{where} \\,\\, \\mu^* = (1 - \\theta_1) (1 - \\theta_2) \\mu \\,\\, \\text{and} \\,\\, \\eta_t = \\epsilon_t - (\\theta_1 + \\theta_2) \\epsilon_{t-1} + \\theta_1\\theta_2 \\epsilon_{t-2}.\n",
    "$$\n",
    "\n",
    "This is just a regression of performance on two lagged values of itself and of the external training input, almost. There is the issue of the MA(2) error structure that was induced by the common filter. We will see that this has real consequences.\n",
    "\n",
    "\n",
    "Good picture:\n",
    "https://en.wikipedia.org/wiki/Dynamic_Bayesian_network#/media/File:R%C3%A9seau_bay%C3%A9sien_simplifi%C3%A9.svg\n",
    "\n",
    "## Example in R\n",
    "\n",
    "This section will use simulated data that can be reproduced from an [R gist](https://gist.github.com/baogorek/6d682e42079005b3bde951e98ebae89e), or downloaded directly as a [csv file](https://drive.google.com/open?id=1kk40wiVYzPXOkrPffU55Vzy-LLTrgAVh). To run the R code below, change the file path in the following R code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n",
      "Attaching package: ‘nlme’\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    collapse\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>day</th><th scope=col>day_of_week</th><th scope=col>period</th><th scope=col>w</th><th scope=col>perf</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1       </td><td>0       </td><td>build-up</td><td>10      </td><td>489.1974</td></tr>\n",
       "\t<tr><td>2       </td><td>1       </td><td>build-up</td><td>40      </td><td>500.5453</td></tr>\n",
       "\t<tr><td>3       </td><td>2       </td><td>build-up</td><td>42      </td><td>479.8866</td></tr>\n",
       "\t<tr><td>4       </td><td>3       </td><td>build-up</td><td>31      </td><td>474.2269</td></tr>\n",
       "\t<tr><td>5       </td><td>4       </td><td>build-up</td><td>46      </td><td>459.3228</td></tr>\n",
       "\t<tr><td>6       </td><td>5       </td><td>build-up</td><td>20      </td><td>467.1399</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       " day & day\\_of\\_week & period & w & perf\\\\\n",
       "\\hline\n",
       "\t 1        & 0        & build-up & 10       & 489.1974\\\\\n",
       "\t 2        & 1        & build-up & 40       & 500.5453\\\\\n",
       "\t 3        & 2        & build-up & 42       & 479.8866\\\\\n",
       "\t 4        & 3        & build-up & 31       & 474.2269\\\\\n",
       "\t 5        & 4        & build-up & 46       & 459.3228\\\\\n",
       "\t 6        & 5        & build-up & 20       & 467.1399\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "day | day_of_week | period | w | perf | \n",
       "|---|---|---|---|---|---|\n",
       "| 1        | 0        | build-up | 10       | 489.1974 | \n",
       "| 2        | 1        | build-up | 40       | 500.5453 | \n",
       "| 3        | 2        | build-up | 42       | 479.8866 | \n",
       "| 4        | 3        | build-up | 31       | 474.2269 | \n",
       "| 5        | 4        | build-up | 46       | 459.3228 | \n",
       "| 6        | 5        | build-up | 20       | 467.1399 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  day day_of_week period   w  perf    \n",
       "1 1   0           build-up 10 489.1974\n",
       "2 2   1           build-up 40 500.5453\n",
       "3 3   2           build-up 42 479.8866\n",
       "4 4   3           build-up 31 474.2269\n",
       "5 5   4           build-up 46 459.3228\n",
       "6 6   5           build-up 20 467.1399"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df <- read.csv(\"/mnt/c/devl/data/train_df.csv\")\n",
    "head(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running any regressions, the following use of R as a calculator shows the theoretical values of all the coefficients in the lagged variable model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.606982842406423"
      ],
      "text/latex": [
       "0.606982842406423"
      ],
      "text/markdown": [
       "0.606982842406423"
      ],
      "text/plain": [
       "[1] 0.6069828"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "1.90943253246393"
      ],
      "text/latex": [
       "1.90943253246393"
      ],
      "text/markdown": [
       "1.90943253246393"
      ],
      "text/plain": [
       "[1] 1.909433"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "-0.910656288194592"
      ],
      "text/latex": [
       "-0.910656288194592"
      ],
      "text/markdown": [
       "-0.910656288194592"
      ],
      "text/plain": [
       "[1] -0.9106563"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "-0.181166489465912"
      ],
      "text/latex": [
       "-0.181166489465912"
      ],
      "text/markdown": [
       "-0.181166489465912"
      ],
      "text/plain": [
       "[1] -0.1811665"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "0.182131257638918"
      ],
      "text/latex": [
       "0.182131257638918"
      ],
      "text/markdown": [
       "0.182131257638918"
      ],
      "text/plain": [
       "[1] 0.1821313"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# True parameter values:\n",
    "mu <- 496\n",
    "theta1 <- exp(-1 / 60)\n",
    "theta2 <- exp(-1 / 13)\n",
    "k1 <- .07\n",
    "k2 <- .27\n",
    "\n",
    "#Theoretical coefficient for intercept is\n",
    "(1 - theta1) * (1 - theta2) * mu\n",
    "\n",
    "#Theoretical coefficient for performance lagged once is\n",
    "theta1 + theta2\n",
    "\n",
    "#Theoretical coefficient for performance lagged twice is\n",
    "-theta1 * theta2\n",
    "\n",
    "#Theoretical coefficient for training lagged once is\n",
    "k1 * theta1 - k2 * theta2\n",
    "\n",
    "#Theoretical coefficient for training lagged twice is\n",
    "-theta1 * theta2 * (k1 - k2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the goal is to recover the following equation via lagged variable regression:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&p_t = .6070 + 1.9094 p_{t - 1} - .9107 p_{t - 2} - .1812 w_{t-1} + .1821 w_{t-2} + \\eta_t, \\\\\n",
    "&  \\text{where}\\, \\, \\eta_t = \\epsilon_t - 1.9094 \\epsilon_{t-1} + .9107 \\epsilon_{t-2}.\n",
    "\\end{aligned}\n",
    "$$ Note that the roots of the characteristic polynomial for both the AR component and MA component is\n",
    "$$\n",
    "1 - 1.9094 x + .9107 x^2 = 0\n",
    "$$\n",
    "which has real roots of 1.08 and 1.02, both larger than 1; the AR component is stationary and the MA component is invertible, **barely**. Still, given that the performance is being driven by external training factors, the model still depends heavily on where the athelete is in time, i.e., $t$, and hence the model unconditionally is still nonstationary. But it is conditionally stationary given $w$.\n",
    "\n",
    "## Regression time\n",
    "To prepare for the regression, the code below lags performance and training variables once and twice each. A shout out is in order to dplyr for making this so easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(dplyr)\n",
    "\n",
    "train_aug <- train_df %>%\n",
    "  mutate(perf_lag1 = lag(perf, n = 1, order_by = day),\n",
    "         perf_lag2 = lag(perf, n = 2, order_by = day),\n",
    "         train_lag1 = lag(w, n = 1, order_by = day),\n",
    "         train_lag2 = lag(w, n = 2, order_by = day))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I think about OLS regression with lagged variables, I usually am not that concerned. I remember, for instance, that the OLS regression estimate for an AR(1) model is strongly consistent. It is not fully efficient because a regressor is correlated with *past* residuals, but it's still consistent. So let's see what happens when we regress performance on these lagged variables using OLS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = perf ~ perf_lag1 + perf_lag2 + train_lag1 + train_lag2, \n",
       "    data = train_aug[3:nrow(train_aug), ])\n",
       "\n",
       "Residuals:\n",
       "     Min       1Q   Median       3Q      Max \n",
       "-21.0297  -6.3896   0.3202   5.7529  25.3549 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept) 33.99821   12.09470   2.811 0.005327 ** \n",
       "perf_lag1    0.48066    0.05619   8.553 1.18e-15 ***\n",
       "perf_lag2    0.46189    0.05504   8.393 3.45e-15 ***\n",
       "train_lag1  -0.15602    0.04406  -3.541 0.000475 ***\n",
       "train_lag2  -0.02346    0.04516  -0.520 0.603807    \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 8.518 on 252 degrees of freedom\n",
       "Multiple R-squared:  0.9117,\tAdjusted R-squared:  0.9103 \n",
       "F-statistic: 650.4 on 4 and 252 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_lm <- lm(perf ~ perf_lag1 + perf_lag2 + train_lag1 + train_lag2,\n",
    "            data = train_aug[3:nrow(train_aug), ])\n",
    "summary(my_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, what's going on here? There is clearly bias, and substantial bias at that, in these estimates. This leads to a hugely imporant point: *OLS regression is consistent for dynamic regressions when there is no serial correlation remaining in the residuals.* The world \"remaining\" is somewhat redundant, because they are residuals, but I use it to emphasise that while the AR and MA components of a traditional ARMA model are meant to remove serial correlation from the residuals (i.e., \"whitening\"), if you didn't do a perfect job, there will be bias. This is because the lagged regressors are now *contemporanesly correlated* with the regression error - the definition of \"endogeneity.\"\n",
    "\n",
    "In this case, we were guaranteed to have serial correlation in the residuals because of our common filter operation. The common filter gave us a nice reduced form; now we have to pay the piper. Fortunately, R's `nlme` package offers generalized least squares model fitting via the `gls` function, which handle our (barely invertible) MA error structure out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generalized least squares fit by REML\n",
       "  Model: perf ~ perf_lag1 + perf_lag2 + train_lag1 + train_lag2 \n",
       "  Data: train_aug[3:nrow(train_aug), ] \n",
       "       AIC      BIC    logLik\n",
       "  1804.258 1832.493 -894.1288\n",
       "\n",
       "Correlation Structure: ARMA(0,2)\n",
       " Formula: ~day \n",
       " Parameter estimate(s):\n",
       "    Theta1     Theta2 \n",
       "-1.9059497  0.9117409 \n",
       "\n",
       "Coefficients:\n",
       "                 Value  Std.Error    t-value p-value\n",
       "(Intercept)  0.6571088 0.11700730    5.61596       0\n",
       "perf_lag1    1.9187158 0.00815689  235.22646       0\n",
       "perf_lag2   -0.9200058 0.00815495 -112.81568       0\n",
       "train_lag1  -0.1662026 0.02238219   -7.42566       0\n",
       "train_lag2   0.1664704 0.02241510    7.42671       0\n",
       "\n",
       " Correlation: \n",
       "           (Intr) prf_l1 prf_l2 trn_l1\n",
       "perf_lag1   0.054                     \n",
       "perf_lag2  -0.076 -1.000              \n",
       "train_lag1 -0.156  0.357 -0.352       \n",
       "train_lag2  0.115 -0.372  0.368 -0.999\n",
       "\n",
       "Standardized residuals:\n",
       "          Min            Q1           Med            Q3           Max \n",
       "-2.6183709992 -0.6998483434  0.0001305954  0.7173131454  2.7331771925 \n",
       "\n",
       "Residual standard error: 16.45148 \n",
       "Degrees of freedom: 257 total; 252 residual"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(nlme)\n",
    "my_gls <- gls(perf ~ perf_lag1 + perf_lag2 + train_lag1 + train_lag2,\n",
    "              data = train_aug[3:nrow(train_aug), ],\n",
    "              corARMA(form = ~day, p = 0, q = 2))\n",
    "summary(my_gls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we've now recovered the true parameter values, at least up to plausible estimation error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
